# Deep Analysis: Why the Poller is Always an Issue

**Date:** 2026-01-24  
**Analysis Type:** Architectural Complexity Analysis  
**Status:** ðŸ” **ROOT CAUSES IDENTIFIED**

---

## Executive Summary

The message poller is **fundamentally overcomplicated** due to **architectural design decisions** that create:
1. **Too many layers of abstraction** (7+ services interacting)
2. **Unclear transaction boundaries** (each service manages its own DB sessions)
3. **Async/Sync mixing** (async poller calling sync services via executors)
4. **Connection pool exhaustion** (multiple services creating sessions independently)
5. **Complex error recovery** (errors can occur at any layer, hard to trace)
6. **Leader election overhead** (separate heartbeat loop, database locking)

**The poller is not inherently unreliable - it's just too complex to maintain and debug.**

---

## 1. Architectural Complexity: Too Many Moving Parts

### Current Architecture (7+ Services):

```
MessagePollerService (async)
  â”œâ”€â”€ BackgroundTaskLeader (async heartbeat loop)
  â”‚   â””â”€â”€ Connection pool monitor (sync check)
  â”œâ”€â”€ IntegrationInboxService (sync, run in executor)
  â”‚   â””â”€â”€ Creates its own DB sessions
  â”œâ”€â”€ StatusUpdateService (sync, run in executor)
  â”‚   â””â”€â”€ Creates its own DB sessions
  â”œâ”€â”€ StuckJobReclaimer (sync, run in executor)
  â”‚   â””â”€â”€ Creates its own DB sessions
  â””â”€â”€ DocumentProcessor (sync, run in executor)
      â”œâ”€â”€ BlobStorageClient (sync)
      â”œâ”€â”€ DocumentSplitter (sync)
      â”œâ”€â”€ OCRService (sync, HTTP calls)
      â”œâ”€â”€ CoversheetDetector (sync)
      â”œâ”€â”€ PartClassifier (sync)
      â””â”€â”€ Creates its own DB sessions
```

### Problems:

1. **7+ different services** each managing their own database sessions
2. **No unified transaction management** - each service commits independently
3. **Connection pool exhaustion** - each service grabs connections independently
4. **Error handling complexity** - errors can occur at any layer
5. **Hard to debug** - which service failed? Which transaction rolled back?

---

## 2. Database Session Management Chaos

### Current Pattern (PROBLEMATIC):

```python
# MessagePollerService (async)
inbox_service = IntegrationInboxService()  # Creates session internally
messages = await loop.run_in_executor(None, inbox_service.poll_new_messages, ...)
# Session still open in inbox_service

# For each job:
inbox_service = IntegrationInboxService()  # NEW session
job = await loop.run_in_executor(None, inbox_service.claim_job, ...)
# Another session

# DocumentProcessor (sync)
with get_db_session() as db:  # YET ANOTHER session
    processor.process_message(message, inbox_id=inbox_id)
    # Long-running operation holding connection
```

### Issues:

1. **Multiple sessions per poll cycle:**
   - 1 session for `poll_new_messages`
   - 1 session per `insert_into_inbox` (7 messages = 7 sessions)
   - 1 session for `update_watermark`
   - 1 session per `claim_job` (up to 5 jobs = 5 sessions)
   - 1 session per `get_source_message`
   - 1 session per `mark_done_with_retry` (with retries = multiple sessions)
   - 1 session in `DocumentProcessor.process_message` (long-running)

2. **Sessions not properly closed:**
   - `IntegrationInboxService` has complex session management (`_get_db`, `fresh=True`)
   - Sessions may leak if exceptions occur
   - `inbox_service.close()` called in `finally`, but what if exception before that?

3. **Long-running operations hold connections:**
   - `DocumentProcessor.process_message()` can take minutes (OCR, blob operations)
   - Connection held for entire duration
   - Blocks other operations from getting connections

4. **No connection pooling awareness:**
   - Each service blindly creates sessions
   - No coordination between services
   - Pool exhaustion happens silently

---

## 3. Async/Sync Mixing Creates Complexity

### Current Pattern:

```python
# Async poller loop
async def _poll_loop(self):
    await self._poll_and_process()  # async

async def _poll_and_process(self):
    # Run sync service in executor
    messages = await loop.run_in_executor(
        None, 
        inbox_service.poll_new_messages,  # sync
        batch_size
    )
    
    # For each message, run sync insert in executor
    inbox_id = await loop.run_in_executor(
        None,
        inbox_service.insert_into_inbox,  # sync
        ...
    )
    
    # Process job - runs sync DocumentProcessor in executor
    await self._process_message(...)  # async, but calls sync processor
```

### Problems:

1. **Error propagation complexity:**
   - Sync exceptions wrapped in async context
   - Stack traces become confusing
   - Hard to debug which layer failed

2. **Transaction boundaries unclear:**
   - Sync service commits in executor
   - Async code doesn't know if commit succeeded
   - No way to coordinate transactions across async/sync boundary

3. **Connection pool issues:**
   - Executor threads may hold connections longer
   - No visibility into executor thread connection usage
   - Pool exhaustion harder to detect

4. **Cancellation complexity:**
   - Can't easily cancel sync operations running in executor
   - Long-running sync operations block graceful shutdown

---

## 4. Transaction Management: No Clear Boundaries

### Current Pattern (PROBLEMATIC):

```python
# IntegrationInboxService.insert_into_inbox()
db = self._get_db(fresh=True)  # New session
try:
    db.execute(INSERT ...)
    db.commit()  # Commit 1
except:
    db.rollback()
finally:
    # Session may not be closed immediately

# DocumentProcessor.process_message()
with get_db_session() as db:  # Another session
    # Multiple operations
    db.add(packet)
    db.flush()
    db.add(document)
    db.commit()  # Commit 2

# StatusUpdateService.mark_done_with_retry()
db = SessionLocal()  # Yet another session
try:
    db.execute(UPDATE ...)
    db.commit()  # Commit 3
except:
    db.rollback()
```

### Issues:

1. **Multiple commits per message:**
   - Insert into inbox: 1 commit
   - Process message: 1 commit
   - Mark done: 1 commit (with retries = multiple commits)
   - **Total: 3+ commits per message**

2. **No atomicity:**
   - If `mark_done` fails, message is processed but status not updated
   - Requires retry logic (`mark_done_with_retry`)
   - Adds complexity

3. **No rollback coordination:**
   - If `DocumentProcessor` fails, inbox insert already committed
   - Message stuck in inbox, needs retry
   - No way to rollback the entire operation

4. **Connection held across commits:**
   - Each commit releases connection briefly
   - But new session grabbed immediately
   - Pool thrashing

---

## 5. Connection Pool Exhaustion: Root Cause

### Why Pool Exhaustion Happens:

1. **Too many concurrent operations:**
   - Poller processes up to 5 jobs concurrently
   - Each job creates multiple sessions
   - Each session holds connection for duration of operation

2. **Long-running operations:**
   - `DocumentProcessor.process_message()` can take 2-5 minutes
   - Connection held for entire duration
   - 5 concurrent jobs = 5 connections held for minutes

3. **No connection lifecycle management:**
   - Services create sessions without checking pool
   - No backpressure mechanism
   - Pool exhausted silently

4. **Heartbeat competes for connections:**
   - Heartbeat loop needs connection every 30s
   - If pool exhausted, heartbeat fails
   - Leadership lost

### Example Scenario:

```
Pool size: 80
Max overflow: 120
Total: 200 connections

Concurrent operations:
- 5 jobs processing (5 connections)
- Each job: DocumentProcessor (1 connection, 2-5 min)
- Each job: StatusUpdateService retries (1 connection per retry)
- Heartbeat (1 connection every 30s)
- Other API requests (10-20 connections)

If 5 jobs each hold connection for 3 minutes:
- 5 connections Ã— 3 minutes = 15 connection-minutes
- But if jobs overlap, need 5 connections simultaneously
- Plus heartbeat, plus API requests
- Pool can exhaust quickly
```

---

## 6. Error Recovery Complexity

### Current Error Handling:

```python
try:
    await self._poll_and_process()
except Exception as e:
    logger.error(f"Error in poll and process: {e}", exc_info=True)
    # Continue loop - will retry on next iteration
```

### Problems:

1. **Errors swallowed:**
   - Exceptions logged but processing continues
   - No way to know if specific message failed
   - Messages may be stuck

2. **Partial failures:**
   - Message inserted into inbox but processing failed
   - Status not updated
   - Requires stuck job reclaimer to fix

3. **No error categorization:**
   - Transient errors (DB timeout) vs permanent errors (invalid payload)
   - Both handled the same way
   - Permanent errors retry forever

4. **Complex retry logic:**
   - `StatusUpdateService.mark_done_with_retry()` has its own retry logic
   - `StuckJobReclaimer` has its own recovery logic
   - Multiple retry mechanisms conflict

---

## 7. Leader Election Overhead

### Current Implementation:

```python
# Separate async task for heartbeat
self.heartbeat_task = asyncio.create_task(self._heartbeat_loop())

# Heartbeat loop runs every 30s
async def _heartbeat_loop(self):
    while self.is_leader:
        await asyncio.sleep(30)
        # Check connection pool
        # Update heartbeat in DB
        # Handle errors with exponential backoff
```

### Problems:

1. **Additional complexity:**
   - Separate async task to manage
   - Separate error handling
   - Separate connection pool checks

2. **Database overhead:**
   - UPDATE query every 30 seconds
   - Additional connection usage
   - Competes with processing for connections

3. **Failure modes:**
   - Heartbeat can fail independently of processing
   - Leadership lost even if processing works
   - Complex recovery logic needed

4. **Startup complexity:**
   - Must start heartbeat task
   - Must verify heartbeat works
   - Additional startup checks needed

---

## 8. IntegrationInboxService: Overcomplicated Session Management

### Current Pattern:

```python
class IntegrationInboxService:
    def __init__(self, db: Optional[Session] = None):
        self._db = db
        self._own_session = db is None
    
    def _get_db(self, fresh: bool = False) -> Session:
        if fresh or self._db is None:
            if self._db and self._own_session:
                try:
                    self._db.close()
                except Exception:
                    pass
            self._db = SessionLocal()
            self._own_session = True
        return self._db
```

### Problems:

1. **Complex session lifecycle:**
   - Tracks if it "owns" session
   - Can create "fresh" sessions
   - Session may or may not be closed
   - Hard to reason about

2. **Session leaks:**
   - If exception occurs, session may not be closed
   - `close()` called in `finally`, but what if exception before that?
   - Multiple code paths, easy to miss cleanup

3. **Unclear ownership:**
   - Can accept external session
   - Can create own session
   - Unclear when to close

4. **Not thread-safe:**
   - Multiple executors may call same instance
   - Session management not thread-safe
   - Race conditions possible

---

## 9. DocumentProcessor: Monolithic and Long-Running

### Current Implementation:

```python
def process_message(self, message, inbox_id):
    # 1. Parse payload
    # 2. Get/create packet (DB operation)
    # 3. Download documents from blob (network I/O)
    # 4. Merge documents (file I/O)
    # 5. Upload consolidated PDF (network I/O)
    # 6. Split PDF (CPU intensive)
    # 7. Upload pages (network I/O)
    # 8. Run OCR on all pages (HTTP calls, can take minutes)
    # 9. Detect coversheet (CPU)
    # 10. Classify Part A/B (CPU)
    # 11. Update database (DB operation)
    # 12. Commit transaction
```

### Problems:

1. **Single long transaction:**
   - Entire operation in one transaction
   - Connection held for 2-5 minutes
   - Blocks other operations

2. **Mixed I/O types:**
   - Database I/O
   - Blob storage I/O
   - HTTP calls (OCR)
   - File I/O
   - All in one function
   - Hard to optimize

3. **No progress tracking:**
   - If fails at step 8 (OCR), all previous work lost
   - No way to resume
   - Must restart from beginning

4. **Error handling complexity:**
   - Errors can occur at any step
   - Different error types need different handling
   - Complex try/except blocks

---

## 10. StatusUpdateService: Retry Logic Adds Complexity

### Current Pattern:

```python
def mark_done_with_retry(self, inbox_id: int) -> RetryResult:
    for attempt in range(max_attempts):
        try:
            db = SessionLocal()  # New session per retry
            db.execute(UPDATE ...)
            db.commit()
            return RetryResult(success=True)
        except Exception:
            db.rollback()
            await asyncio.sleep(backoff_delay)
    return RetryResult(success=False)
```

### Problems:

1. **Multiple sessions per update:**
   - Each retry creates new session
   - If 3 retries needed, 3 sessions used
   - Wastes connection pool

2. **Retry logic in multiple places:**
   - `StatusUpdateService` has retry logic
   - `StuckJobReclaimer` has retry logic
   - Poller has retry logic
   - Conflicting retry mechanisms

3. **No idempotency guarantees:**
   - Retry may succeed but message already processed
   - No way to check if update already applied
   - May cause duplicate processing

---

## 11. The Real Problem: Architectural Over-Engineering

### What Should Be Simple:

```
1. Poll for new messages
2. Insert into inbox (idempotent)
3. Claim job from inbox
4. Process job
5. Mark job done
```

### What It Actually Is:

```
1. Poll for new messages (IntegrationInboxService, own session)
2. Check connection pool (ConnectionPoolMonitor)
3. Insert into inbox (IntegrationInboxService, own session, per message)
4. Update watermark (IntegrationInboxService, own session)
5. Claim job (IntegrationInboxService, own session, per job)
6. Get source message (IntegrationInboxService, own session)
7. Process message (DocumentProcessor, own session, long-running)
   - Parse payload
   - Download blobs
   - Merge PDFs
   - Split pages
   - Upload blobs
   - Run OCR (HTTP calls)
   - Update database
8. Mark done (StatusUpdateService, own session, with retries)
9. Heartbeat (BackgroundTaskLeader, own session, every 30s)
10. Stuck job reclaimer (StuckJobReclaimer, own session, periodically)
```

### Complexity Metrics:

- **Services involved:** 7+
- **Database sessions per message:** 5-10
- **Database commits per message:** 3-5
- **Async/sync boundaries:** 5+
- **Error handling layers:** 3+
- **Retry mechanisms:** 3 different systems

---

## 12. Specific Issues from Logs

### From the provided logs:

1. **"RuntimeError: No response returned"** errors:
   - These are ASGI/Starlette errors, not poller errors
   - But they occur during poller operation
   - Suggests request handling issues, possibly related to long-running operations

2. **Poller continues processing:**
   - Errors don't stop poller
   - Good for resilience, but masks issues
   - Hard to know if errors are critical

3. **No clear transaction boundaries:**
   - Multiple commits per message
   - If one fails, others may have succeeded
   - Inconsistent state possible

---

## 13. Root Causes Summary

### Why Poller is Always an Issue:

1. **Too many services** (7+) each managing their own state
2. **Unclear transaction boundaries** - no single source of truth
3. **Connection pool exhaustion** - too many sessions, long-running operations
4. **Async/sync mixing** - complexity in error handling and transaction management
5. **No unified error handling** - errors can occur at any layer
6. **Complex session management** - each service manages its own sessions
7. **Leader election overhead** - separate async task, additional complexity
8. **Monolithic processing** - DocumentProcessor does too much in one transaction
9. **Multiple retry mechanisms** - conflicting retry logic
10. **Hard to debug** - errors can occur at any layer, hard to trace

---

## 14. What Makes It Complicated: The Core Issues

### Issue #1: Too Many Abstractions

**Problem:** Each service is a separate abstraction with its own:
- Database session management
- Error handling
- Retry logic
- Transaction boundaries

**Impact:** Hard to reason about, hard to debug, hard to maintain

### Issue #2: No Unified Transaction Management

**Problem:** Each service commits independently:
- No way to rollback entire operation
- Partial failures create inconsistent state
- Requires complex retry logic

**Impact:** Data inconsistency, complex error recovery

### Issue #3: Connection Pool Not Coordinated

**Problem:** Each service grabs connections independently:
- No awareness of pool state
- Long-running operations hold connections
- Pool exhaustion happens silently

**Impact:** Heartbeat failures, processing failures, leadership loss

### Issue #4: Async/Sync Boundary Complexity

**Problem:** Async poller calls sync services via executors:
- Error propagation unclear
- Transaction boundaries unclear
- Connection lifecycle unclear

**Impact:** Hard to debug, hard to optimize

### Issue #5: Monolithic Processing

**Problem:** DocumentProcessor does everything in one function:
- Long-running transaction
- Mixed I/O types
- No progress tracking

**Impact:** Connection held for minutes, no way to resume on failure

---

## 15. Why It's Hard to Fix

### Current Fixes Are Band-Aids:

1. **Connection pool size increase** - Doesn't solve root cause, just delays exhaustion
2. **Heartbeat pool checks** - Workaround, not solution
3. **Graceful shutdown** - Fixes symptom, not cause
4. **Exponential backoff** - Makes failures slower, doesn't prevent them

### Real Fixes Would Require:

1. **Architectural redesign** - Too risky, too much work
2. **Unified transaction management** - Breaks existing code
3. **Simplified service layer** - Requires refactoring all services
4. **Better connection lifecycle** - Requires changing all services

### Why We Keep Adding Complexity:

- Each fix adds more code
- More code = more complexity
- More complexity = more bugs
- More bugs = more fixes
- **Vicious cycle**

---

## 16. The Fundamental Question

### Is the Poller Actually Broken?

**Answer: No, but it's fragile.**

The poller works, but:
- Fails under load (connection pool exhaustion)
- Fails on errors (complex error recovery)
- Fails on deployment (graceful shutdown issues)
- Hard to debug (too many layers)
- Hard to maintain (too much complexity)

### What Would Make It Reliable?

1. **Simpler architecture** - Fewer services, clearer boundaries
2. **Unified transaction management** - Single transaction per message
3. **Better connection lifecycle** - Explicit connection management
4. **Progress tracking** - Resume on failure
5. **Clear error handling** - Categorize errors, handle appropriately

---

## 17. Recommendations (Analysis Only - No Code Changes)

### Short-Term (Keep Current Architecture):

1. **Monitor connection pool usage** - Add metrics
2. **Add circuit breaker** - Stop processing when pool critical
3. **Simplify error handling** - Single error handling layer
4. **Add progress tracking** - Resume processing on failure

### Long-Term (Architectural Changes):

1. **Unified transaction manager** - Single transaction per message
2. **Simplified service layer** - Combine related services
3. **Better connection lifecycle** - Explicit connection management
4. **Progress tracking** - Save state, resume on failure
5. **Clear error categorization** - Transient vs permanent errors

### Radical (Complete Redesign):

1. **Event-driven architecture** - Messages trigger events
2. **Separate processing workers** - Poller just enqueues
3. **Queue-based processing** - Use message queue (Azure Service Bus)
4. **Stateless processing** - No long-running transactions

---

## 18. Conclusion

### Why Poller is Always an Issue:

**The poller is not broken - it's overcomplicated.**

The complexity comes from:
1. **Too many services** (7+) interacting
2. **Unclear transaction boundaries** (each service commits independently)
3. **Connection pool exhaustion** (too many sessions, long-running operations)
4. **Async/sync mixing** (complexity in error handling)
5. **Monolithic processing** (everything in one transaction)

### The Real Problem:

**Architectural over-engineering** - What should be simple (poll â†’ process â†’ done) has become a complex web of services, each managing their own state, transactions, and error handling.

### The Solution:

**Simplification** - Reduce services, unify transaction management, better connection lifecycle, progress tracking.

**But this requires architectural changes, not just fixes.**

---

**Analysis Date:** 2026-01-24  
**Analyst:** Deep Code Analysis  
**Status:** âœ… **ROOT CAUSES IDENTIFIED - NO CODE CHANGES MADE**
